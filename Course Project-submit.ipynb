{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Liang Tianyi；Chen Meng\n",
    "\n",
    "**EIDs:** 58276105; 58340501\n",
    "\n",
    "**Kaggle Competition:** Optiver - Trading at the Close\n",
    "\n",
    "**Kaggle Team Name:** C&L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Course Project (2023A)\n",
    "\n",
    "Due date: See canvas site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Projects\n",
    "\n",
    "For the course project, you may select **one** of the following competitions on Kaggle **or** define your own course project:\n",
    "\n",
    "### [LLM - Detect AI Generated Text](https://www.kaggle.com/competitions/llm-detect-ai-generated-text): Identify which essay was written by a large language model\n",
    "\n",
    ">In recent years, large language models (LLMs) have become increasingly sophisticated, capable of generating text that is difficult to distinguish from human-written text. In this competition, we hope to foster open research and transparency on AI detection techniques applicable in the real world.\n",
    ">\n",
    ">This competition challenges participants to develop a machine learning model that can accurately detect whether an essay was written by a student or an LLM. The competition dataset comprises a mix of student-written essays and essays generated by a variety of LLMs.\n",
    "\n",
    "### [Optiver - Trading at the Close](https://www.kaggle.com/competitions/optiver-trading-at-the-close): Predict US stocks closing movements\n",
    "\n",
    ">In this competition, you are challenged to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities.\n",
    "\n",
    "### [Linking Writing Processes to Writing Quality](https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality): Use typing behavior to predict essay quality identify which essay was written by a large language model\n",
    "\n",
    ">The goal of this competition is to predict overall writing quality. Does typing behavior affect the outcome of an essay? You will develop a model trained on a large dataset of keystroke logs that have captured writing process features.\n",
    ">\n",
    ">Your work will help explore the relationship between learners’ writing behaviors and writing performance, which could provide valuable insights for writing instruction, the development of automated writing evaluation techniques, and intelligent tutoring systems.\n",
    "\n",
    "### [Child Mind Institute - Detect Sleep States](https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states): Detect sleep onset and wake from wrist-worn accelerometer data\n",
    "\n",
    "> Your work will improve researchers' ability to analyze accelerometer data for sleep monitoring and enable them to conduct large-scale studies of sleep. Ultimately, the work of this competition could improve awareness and guidance surrounding the importance of sleep. The valuable insights into how environmental factors impact sleep, mood, and behavior can inform the development of personalized interventions and support systems tailored to the unique needs of each child.\n",
    "\n",
    "\n",
    "### [Enefit - Predict Energy Behavior of Prosumers](https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers): Predict Prosumer Energy Patterns and Minimize Imbalance Costs.\n",
    "\n",
    "> The goal of the competition is to create an energy prediction model of prosumers to reduce energy imbalance costs.\n",
    ">\n",
    "> This competition aims to tackle the issue of energy imbalance, a situation where the energy expected to be used doesn't line up with the actual energy used or produced. Prosumers, who both consume and generate energy, contribute a large part of the energy imbalance. Despite being only a small part of all consumers, their unpredictable energy use causes logistical and financial problems for the energy companies.\n",
    "\n",
    "\n",
    "### Student-defined Course Project \n",
    "\n",
    "The goal of the student-defined project is to get some hands-on experience using the course material on your own research problems. Keep in mind that there will only be about 4 weeks to do the project, so the scope should not be too large. Following the major themes of the course, here are some general topics for the project:\n",
    "- _regression_ (supervised learning) - use regression methods (e.g. ridge regression, Gaussian processes) to model data or predict from data.\n",
    "- _classification_ (supervised learning) - use classification methods (e.g., SVM, BDR, Logistic Regression, NNs) to learn to distinguish between multiple classes given a feature vector.\n",
    "- _clustering_ (unsupervised learning) - use clustering methods (e.g., K-means, EM, Mean-Shift) to discover the natural groups in data.\n",
    "- _visualization_ (unsupervised learning) - use dimensionality reduction methods (e.g., PCA, kernel-PCA, non-linear embedding) to visualize the structure of high-dimensional data.\n",
    " \n",
    "You can pick any one of these topics and apply them to your own problem/data. \n",
    "\n",
    "- *Can my project be my recently submitted or soon-to-be submitted paper?* If you plan to just\n",
    "turn in the results from your paper, then the answer is no. The project cannot be be work\n",
    "that you have already done. However, your course project can be based on extending your\n",
    "work. For example, you can try some models introduced in the course on your data/problem.\n",
    "\n",
    "Before actually doing the project, you need to write a **project proposal** so that we can make sure the project is doable within the 3-4 weeks. I can also give you some pointers to relevant methods, if necessary.  \n",
    "- The project proposal should be at most one page with the following contents: 1) an introduction that briefy states the problem; 2) a precise description of what you plan to do - e.g., What types of features do you plan to use? What algorithms do you plan to use? What dataset will you use? How will you evaluate your results? How do you define a good outcome for the project?\n",
    "- The goal of the proposal is to work out, in your head, what your project will be. Once the proposal is done, it is just a matter of implementation!\n",
    "- *You need to submit the project proposal to Canvas 1 week after the Course project is released.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups\n",
    "Group projects should contain 2 students.  To sign up for a group, go to Canvas and under \"People\", join an existing **\"Project Group X\"**, where X is a number.  _For group projects, the project report must state the percentage contribution from each project member. You must also submit the contribution percentages to the \"Project Group Contribution\" assignment on Canvas._\n",
    "\n",
    "## Methodology\n",
    "You are free to choose the methodology to solve the task.  In machine learning, it is important to use domain knowledge to help solve the problem.  Hence, instead of blindly applying the algorithms to the data you need to think about how to represent the data in a way that makes sense for the algorithm to solve the task. \n",
    "\n",
    "## Kaggle: Kaggle Notebooks\n",
    "\n",
    "The Kaggle competitions have Kaggle Notebooks enabled, which provide free GPU/TPU computing resources (up to a limit).  You can develop your model in the Kaggle Notebook, CS5489 JupyterHub (Dive), or on your own computers.\n",
    "\n",
    "## Kaggle: Evaluation on Kaggle\n",
    "\n",
    "For Kaggle projects, the final evaluation will be performed on Kaggle. Note that for these competitions you need to submit your code via the Kaggle Notebook, which will then generate the submission file for processing. \n",
    "\n",
    "## Project Presentation\n",
    "\n",
    "Each project group needs to give a presentation at the end of the semester.  You will record your presentation and upload it to FlipGrid.  The presentation is limited to 5 minutes.  You _must_ give a presentation. See the details in the \"Project Presentations\" Canvas assignment.\n",
    "\n",
    "## What to hand in\n",
    "\n",
    "You need to turn in the following things.\n",
    "\n",
    "The following files should be uploaded to \"Course Project\" on Canvas:\n",
    "\n",
    "1. This ipynb file `CourseProject-2023A.ipynb` with your source code and documentation. **You should write about all the various attempts that you make to find a good solution.** You may also submit .py files, but your documentation should be in the ipynb file.\n",
    "2. A **PDF** version of your ipynb file.\n",
    "3. Presentation slides.\n",
    "4. (Kaggle projects) Your final submission file to Kaggle. Note that most competitions require you to submit the code, and Kaggle will run it on the hidden test set.\n",
    "5. (Kaggle projects) A downloaded copy of your Kaggle Notebook that is submitted to Kaggle. This file should contain the code that generates the final submission file on Kaggle. This code will be used to verify that your Kaggle submission is reproducible.\n",
    "\n",
    "Other things that need to be turned in:\n",
    "- Upload your Project presentation to FlipGrid and the submit the URL to the \"Project Presentations\" assignment on Canvas.  See the detailed instructions in the assignment.\n",
    "- Enter the percentage contribution for each project member using the \"Project Group Contribution\" assignment on Canvas.\n",
    "- (Student-defined projects only) submit your project proposal to the \"Project Proposal\" assignment on Canvas. The project proposal is due 1 week after the course project is released. Kaggle projects do not need to submit a proposal.\n",
    "\n",
    "\n",
    "\n",
    "## Grading\n",
    "The marks of the assignment are distributed as follows:\n",
    "- 40% - Results using various feature representations, dimensionality reduction methods, classifiers, etc.\n",
    "- 25% - Trying out feature representations (e.g. adding additional features, combining features from different sources) or methods not used in the tutorials.\n",
    "- 15% - Quality of the written report.  More points for insightful observations and analysis.\n",
    "- 15% - Project presentation\n",
    "- 5% - For Kaggle projects, final ranking on the Kaggle leaderboard;  For student-defined projects, the project proposal.\n",
    "\n",
    "**Late Penalty:** 25 marks will be subtracted for each day late.\n",
    "\n",
    "**Group contribution:** marks for a group member with less than equal contribution will be deducted according to the following formula:\n",
    "- Let A% and B% be the percentage contributions for group members Alice and Bob. A%+B%=100%\n",
    "- Let x be the group project marks.\n",
    "- If A>B, then Bob's marks will be reduced to be: x*B/A\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR METHODS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis and Data Exploration\n",
    "## Data Inspection\n",
    "Display the first five rows of the training set to examine the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `stock_id`: The unique identifier for a stock. Not all stock IDs exist in every time bucket.\n",
    "2. `date_id`: The unique identifier for a date. Date IDs are continuous and consistent across all stocks.\n",
    "3. `imbalance_size`: The amount (in dollars) of unmatched orders at the current reference price.\n",
    "4. `imbalance_buy_sell`: An indicator reflecting the direction of auction imbalance.\n",
    "   - Buyer imbalance is 1;\n",
    "   - Seller imbalance is -1;\n",
    "   - No imbalance is 0.\n",
    "5. `reference_price`: The price that maximizes matched shares, minimizes imbalance, and minimizes the distance from the midpoint of the buy and sell books.\n",
    "6. `matched_size`: The amount (in dollars) that can be matched at the current reference price.\n",
    "7. `far_price`: The crossing price that maximizes the quantity of shares matched based on auction interest. This calculation excludes continuous market orders.\n",
    "8. `near_price`: The crossing price that maximizes the quantity of shares matched based on both auction and continuous market orders.\n",
    "9. `[bid/ask]_price`: The price of the most competitive buy/sell level in the non-auction book.\n",
    "10. `[bid/ask]_size`: The amount (in dollars) of the most competitive buy/sell level in the non-auction book.\n",
    "11. `wap`: The weighted average price in the non-auction book, calculated as (BidPrice * AskSize + AskPrice * BidSize) / (BidSize + AskSize).\n",
    "12. `seconds_in_bucket`: The number of seconds since the start of the day's closing auction, always starting from 0.\n",
    "13. `target`: The future movement of a stock's wap after 60 seconds, minus the future movement of a synthetic index's wap after 60 seconds. This target is only provided in the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjective Analysis of Feature Implications Based on Experience:\n",
    "\n",
    "- `stock_id`: This is the unique identifier for a stock. This feature may be important for the model, especially if different stocks exhibit different pricing behavior patterns.\n",
    "\n",
    "- `date_id`: This represents the date of the transaction. The date might be correlated with the target variable, but since the model only predicts short-term price movements, this feature is not important.\n",
    "\n",
    "- `seconds_in_bucket`: Market data at a specific time point, which could be an important feature as it reflects market behavior at different times of the day. `imbalance_size`, `imbalance_buy_sell_flag`, `reference_price`, `matched_size`, `far_price`, `near_price`, `bid_price`, `bid_size`, `ask_price`, `ask_size`, `wap`: These are key market indicators that might directly affect or reflect the dynamics of stock prices, hence they might all be important features.\n",
    "\n",
    "- `time_id`: If this represents a specific point in time of the transaction, it might be as unimportant as `date_id` in some cases.\n",
    "\n",
    "- `row_id`: This is an identifier for a row of data. If it is only used for marking rows without any actual business significance, it is useless for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Analysis of Feature Distribution\n",
    "### Part 1: Imbalance Size (`imbalance_size`) and Buy-Sell Imbalance Flag (`imbalance_buy_sell_flag`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "train_set = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\n",
    "\n",
    "# Applying logarithmic transformation\n",
    "train_set['imbalance_size_log'] = np.log1p(train_set['imbalance_size'])\n",
    "\n",
    "# Plotting the histogram of the log-transformed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_set['imbalance_size_log'], bins=50, kde=True)\n",
    "plt.title('Log Transformed Distribution of Imbalance Size')\n",
    "plt.xlabel('Log of Imbalance Size')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the Distribution of Imbalance Size (`imbalance_size`)\n",
    "\n",
    "In the distribution graph of imbalance size (`imbalance_size`), the following points can be observed:\n",
    "\n",
    "- **Peak Area**: The histogram displays a distinct peak area on a logarithmic scale, indicating that the dataset is concentrated within a specific range.\n",
    "\n",
    "- **Long Tail Distribution**: The right side of the chart shows characteristics of a long-tail distribution, suggesting that while larger values of imbalance size are less common, they do exist.\n",
    "\n",
    "- **Possible Outliers**: At the lower end of the logarithmic scale, a very high bar is observed, which might represent a large number of very small imbalance size values or zero values.\n",
    "\n",
    "Based on these observations, here are some suggested next steps for feature processing:\n",
    "\n",
    "- **Outlier Handling**: Due to the presence of a long-tail distribution in the dataset, handling outliers might be necessary. This could be achieved through methods like truncation (e.g., removing values above the 99th percentile) or Winsorization.\n",
    "\n",
    "- **Binning or Segmentation**: For features with a long-tail distribution, improving performance through binning or segmentation is often effective. This involves dividing a continuous numerical feature into several categories.\n",
    "\n",
    "- **Feature Transformation Back to Original Scale**: For model training, maintaining the logarithmic transformation is advisable, as it usually helps in better handling long-tail data. However, for the interpretability of the model's predictions, transforming the output back to the original scale might be required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot for the Buy-Sell Imbalance Flag (imbalance_buy_sell_flag)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='imbalance_buy_sell_flag', data=train_set)\n",
    "plt.title('Imbalance Buy Sell Flag Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the Relationship Between 'Imbalance Buy Sell Flag' and 'Imbalance Size'\n",
    "\n",
    "The 'Imbalance Buy Sell Flag' (`imbalance_buy_sell_flag`) and 'Imbalance Size' (`imbalance_size`) are actually related because the flag indicates the direction of the imbalance, while the size indicates the magnitude of the imbalance. Their combination can provide a complete view of the market order imbalance for the model.\n",
    "\n",
    "##### Models That Automatically Detect Interactions Between Features:\n",
    "\n",
    "- **Tree-based Models**:\n",
    "  - **Decision Trees**: Capable of considering interactions between features during node splits.\n",
    "  - **Random Forests**: Composed of multiple decision trees, each considering a different subset of features, thus capturing a variety of feature combinations.\n",
    "  - **Gradient Boosting Trees**: Such as XGBoost, LightGBM, and CatBoost, these models are particularly powerful in capturing complex interactions through learning a series of trees.\n",
    "\n",
    "- **Deep Learning Models**:\n",
    "  - Such as neural networks, which can learn complex nonlinear relationships between features through multiple hidden layers.\n",
    "\n",
    "##### Models That Do Not Automatically Detect Interactions Between Features:\n",
    "\n",
    "- **Linear Models**:\n",
    "  - Such as Linear Regression and Logistic Regression, which assume a linear relationship between features and the output.\n",
    "  - For these models, if there are interactions between features in the data, it is necessary to manually create interaction terms (e.g., through polynomial features, feature crossing, etc.).\n",
    "\n",
    "- **Support Vector Machines (SVM)**:\n",
    "  - Although non-linear relationships between features can be handled using non-linear kernel functions (like the RBF kernel), SVMs do not automatically consider interactions between features prior to the kernel function mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Reference Price (`reference_price`) and Matched Size (`matched_size`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for Reference Price (reference_price)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='reference_price', y='target', data=train_set)\n",
    "plt.title('Reference Price vs Target')\n",
    "plt.show()\n",
    "\n",
    "# Histogram for Matched Size (matched_size)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_set['matched_size'], kde=True)\n",
    "plt.title('Distribution of Matched Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matched Size Analysis\n",
    "\n",
    "- **Central Tendency**: Most data points are concentrated in the lower value range, suggesting that the majority of matched sizes are small.\n",
    "\n",
    "- **Long Tail Distribution**: The data exhibits a clear right-skew (or positive skew) distribution, indicating the presence of a few extremely large values.\n",
    "\n",
    "- **Possible Outliers**: The long tail on the right side of the distribution hints at potential outliers, i.e., very large matched sizes.\n",
    "\n",
    "#### Suggested Processing Methods for This Distribution:\n",
    "\n",
    "- **Log Transformation**: As with imbalance size, log transformation can help reduce the impact of extreme values for data with a long-tail distribution, making the distribution more normal-like.\n",
    "\n",
    "- **Outlier Handling**: Consider processing extreme outliers, such as by truncating them at a certain percentile (e.g., the 99th percentile).\n",
    "\n",
    "- **Standardization**: After log transformation, standardization (like Z-score normalization) can ensure the model is not adversely affected by features of varying magnitudes.\n",
    "\n",
    "- **Feature Engineering**: Based on business understanding, more detailed feature engineering for matched size may be required, such as creating binned features that represent different segments of matched size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Far Price (`far_price`) and Near Price (`near_price`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for Far Price (far_price)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='far_price', y='target', data=train_set)\n",
    "plt.title('Far Price vs Target')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for Near Price (near_price)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='near_price', y='target', data=train_set)\n",
    "plt.title('Near Price vs Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Far Price (`far_price`)\n",
    "\n",
    "- **Data Concentration**: There are many concentrated data points around lower values of `far_price`, indicating more frequent trading activity in this price range.\n",
    "\n",
    "- **Extreme Values**: The chart shows some extreme values of `target`, especially at lower `far_price` levels. These extremes might represent anomalous trades or data errors.\n",
    "\n",
    "- **Sparse Distribution**: As `far_price` increases, the data points become sparser, suggesting that trades are less frequent in higher price ranges.\n",
    "\n",
    "#### Suggested Data Processing Steps Based on These Observations:\n",
    "\n",
    "- **Outlier Handling**: Further investigation might be needed to determine if those extreme `target` values are valid, or if they should be removed or treated differently in the dataset.\n",
    "\n",
    "- **Data Transformation**: If extreme values of the `target` variable are affecting model learning, consider applying a log transformation or other methods to reduce their impact.\n",
    "\n",
    "- **Focused Analysis**: You might want to focus on specific ranges of `far_price`, for example, by analyzing data points within a certain price range to discern clearer patterns.\n",
    "\n",
    "- **Robust Model Selection**: Consider choosing models that are less sensitive to outliers, such as Random Forests or models with robust loss functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Near Price (`near_price`)\n",
    "\n",
    "- **Data Dense Areas**: Data points are more densely clustered in certain areas of `near_price`, indicating potential increased trading activity around these price ranges.\n",
    "\n",
    "- **Data Distribution**: Compared to \"Far Price vs Target\", data appears to be more tightly clustered in certain areas of `near_price`, suggesting a possibly clearer relationship between `near_price` and the target variable.\n",
    "\n",
    "- **Outliers**: Similar to the previous scatter plot, the target variable shows some extreme values at certain `near_price` levels, which might represent significant price fluctuations or anomalous trades.\n",
    "\n",
    "- **Non-linear Trends**: The data does not seem to exhibit a clear linear trend but might have non-linear relationships. This implies that more complex models might be required to capture this kind of relationship.\n",
    "\n",
    "#### Suggested Data Processing Steps Based on These Observations:\n",
    "\n",
    "- **Outlier Handling**: Further analysis is needed on the extreme values of the target variable to determine if they should be excluded from the dataset or require special treatment.\n",
    "\n",
    "- **Non-linear Models**: Given the potential for non-linear relationships, consider using models capable of capturing non-linearities, such as Random Forests, Gradient Boosting Machines, or Neural Networks.\n",
    "\n",
    "- **Data Transformation**: If extreme values of `near_price` are impacting model performance, consider transforming this feature, such as applying a log transformation.\n",
    "\n",
    "- **Segmented Analysis**: You might want to analyze the relationship between `near_price` and the target variable in more detail across different price ranges, which can be achieved by creating binned variables for price intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Bid Price (`bid_price`), Ask Price (`ask_price`), and Weighted Average Price (`wap`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for Bid Price (bid_price) and Ask Price (ask_price)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_set['bid_price'], color=\"blue\", label=\"Bid Price\", kde=True)\n",
    "sns.histplot(train_set['ask_price'], color=\"red\", label=\"Ask Price\", kde=True)\n",
    "plt.title('Bid and Ask Price Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for Weighted Average Price (wap)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='wap', y='target', data=train_set)\n",
    "plt.title('WAP vs Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from the \"Bid and Ask Price Distribution\" Graph\n",
    "\n",
    "- **High Overlap**: The distributions of Bid Price and Ask Price are tightly overlapped, indicating that the buying and selling prices are very close to each other in most cases.\n",
    "\n",
    "- **Concentrated Distribution**: Both prices show a concentrated distribution with a sharp peak, clustered around a specific price point, likely representing the main trading range in the market.\n",
    "\n",
    "- **Symmetry**: The distributions appear relatively symmetric, suggesting no systematic bias between the bid and ask prices in this market.\n",
    "\n",
    "This type of distribution is often seen in markets with good liquidity and active participation from both buyers and sellers. The close distribution of bid and ask prices might also imply a high degree of information symmetry among market participants.\n",
    "\n",
    "#### Implications for Feature Processing\n",
    "\n",
    "Based on these observations, extensive processing of these features might not be necessary because:\n",
    "\n",
    "- The tight distribution suggests the absence of overly extreme outliers.\n",
    "- The highly overlapping bid and ask prices can be directly used for model training, especially in models capable of capturing minute differences between prices.\n",
    "- The spread between ask and bid prices (Ask Price - Bid Price) could be a useful derived feature, reflecting market buying and selling pressure and liquidity.\n",
    "\n",
    "### Observations from the \"Bid and Ask Price Distribution\" Graph\n",
    "\n",
    "- **High Overlap**: The distributions of Bid Price and Ask Price are tightly overlapped, suggesting that the buying and selling prices are very close to each other in most cases.\n",
    "\n",
    "- **Concentrated Distribution**: Both prices show a highly concentrated distribution with a sharp peak, clustering around a specific price point. This could indicate the main trading range in the market.\n",
    "\n",
    "- **Symmetry**: The distributions appear relatively symmetric, indicating no systematic bias between the bid and ask prices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Correlation Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = train_set.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from the Correlation Heatmap\n",
    "\n",
    "- **High Correlation**: Darker cells represent a strong correlation between two variables. For example, the correlation between `bid_price` and `ask_price` is very high, likely because these prices are usually closely linked. Similarly, `wap` (Weighted Average Price) may have a high correlation with both `bid_price` and `ask_price`.\n",
    "\n",
    "- **Low or No Correlation**: Lighter cells indicate lower or almost no correlation between variables, suggesting that these variables may provide relatively independent information.\n",
    "\n",
    "- **Target Variable**: The target variable (`target`) shows low correlation, indicating that no single feature has a very strong linear relationship with the target. This suggests that predicting the target variable might require a combination of multiple features or considering interactions between features.\n",
    "\n",
    "- **Self-correlation**: Cells on the diagonal represent the correlation of variables with themselves, which is always 1.\n",
    "\n",
    "- **Log-Transformed Imbalance Size**: You may have added `imbalance_size_log` as a new feature, which helps address the long-tail distribution of the original imbalance size (`imbalance_size`).\n",
    "\n",
    "#### Steps Based on These Observations:\n",
    "\n",
    "- **Feature Selection**: Choose features that have a higher correlation with the target variable, as these features might be more valuable for prediction.\n",
    "\n",
    "- **Avoiding Multicollinearity**: If there are several highly correlated features, consider retaining only some of them to avoid multicollinearity issues during model training.\n",
    "\n",
    "- **Non-linear Relationships and Interactions**: Explore non-linear relationships and interactions between variables, especially those with moderate correlation with the target variable.\n",
    "\n",
    "- **Model Selection**: As no single feature has a strong linear relationship with the target variable, models that can capture complex interactions between features, such as Random Forests or Neural Networks, might be required.\n",
    "\n",
    "- **Further Analysis**: Conduct further analysis on features with moderate correlation to the target variable to understand if they might influence the target in a non-linear manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- **Data Cleaning**:\n",
    "  - **Remove or Impute Missing Values**: If some features have missing values, you can choose to fill them (e.g., using the median) or remove rows containing missing values.\n",
    "  - **Handling Outliers**: For outliers in features like `target`, `imbalance_size`, you may choose to truncate them to the 99th percentile, or cap them within a certain range.\n",
    "\n",
    "- **Variable Transformation**:\n",
    "  - **Log Transformation**: For long-tail distributed features like `imbalance_size` and `matched_size`, apply log transformation to reduce the impact of extreme values.\n",
    "  \n",
    "  - **Standardization**: Standardize all numerical features (Z-score normalization), especially necessary for linear models and distance-based models.\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  - **Remove Columns in Training and Validation Sets**: Remove 'row_id', 'time_id', 'date_id' columns. These features do not directly contribute to predicting the target or may lead to data leakage.\n",
    "\n",
    "- **Feature Construction**:\n",
    "  - **Price Difference Feature**: Calculate the difference between `ask_price` and `bid_price` as a new feature to capture the market pressure difference between buyers and sellers.\n",
    "  - **Interaction Features**: Consider creating interaction features between `imbalance_size` and `imbalance_buy_sell_flag` to integrate these two related pieces of information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 读取训练集\n",
    "train_set = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\n",
    "\n",
    "# 首先删除包含目标变量缺失值的行\n",
    "train_set = train_set.dropna(subset=['target'])\n",
    "\n",
    "# 选择数值类型的列来计算中位数\n",
    "numerical_cols = train_set.select_dtypes(include=[np.number]).columns\n",
    "median_values = train_set[numerical_cols].median()\n",
    "\n",
    "# 数据清洗\n",
    "# 填充数值类型列的缺失值\n",
    "train_set[numerical_cols] = train_set[numerical_cols].fillna(median_values)\n",
    "\n",
    "# 处理离群值\n",
    "for col in ['imbalance_size']:\n",
    "    upper_limit = train_set[col].quantile(0.99)\n",
    "    train_set[col] = np.where(train_set[col] > upper_limit, upper_limit, train_set[col])\n",
    "\n",
    "# 变量变换\n",
    "# 对长尾分布的特征进行对数变换\n",
    "for col in ['imbalance_size']:\n",
    "    train_set[col + '_log'] = np.log(train_set[col] + 1e-9)\n",
    "\n",
    "# 特征选择\n",
    "# 删除不需要的列\n",
    "train_set.drop(['row_id', 'time_id', 'date_id'], axis=1, inplace=True)\n",
    "\n",
    "# 特征构建\n",
    "# 创建买卖价差特征\n",
    "train_set['bid_ask_spread'] = train_set['ask_price'] - train_set['bid_price']\n",
    "# 创建买卖不平衡方向与大小的交互特征\n",
    "train_set['imbalance_flag_size'] = train_set['imbalance_buy_sell_flag'] * train_set['imbalance_size_log']\n",
    "\n",
    "# 标准化，仅针对连续数值特征\n",
    "continuous_features = train_set.select_dtypes(include=['float64', 'int64']).columns\n",
    "continuous_features = continuous_features.drop(['imbalance_buy_sell_flag', 'stock_id','target'])  # 排除分类特征\n",
    "scaler = StandardScaler()\n",
    "train_set[continuous_features] = scaler.fit_transform(train_set[continuous_features])\n",
    "\n",
    "# 重新保存修改后的数据集\n",
    "train_set.to_csv('/kaggle/working/train_modified.csv', index=False)\n",
    "\n",
    "print('Modified train set has been saved successfully.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## Linear Regression Model\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Initially, a simple attempt is made using a linear regression model to test the accuracy of predictions under linear regression.\n",
    "\n",
    "Mean Squared Error (MSE) is a commonly used metric to evaluate the performance of regression models. It calculates the average of the squared differences between the model's predicted values and actual values. The definition of MSE is as follows:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where \\( n \\) is the number of samples, \\( y_i \\) is the actual value of the \\( i \\)th observation, and \\( \\hat{y}_i \\) is the model's predicted value for the \\( i \\)th observation.\n",
    "The smaller the value of MSE, the closer the model's predictions are to the actual values, indicating better performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('/kaggle/working/train_modified.csv')\n",
    "\n",
    "# Split features and target variable\n",
    "X = train_data.drop(['target'], axis=1)\n",
    "y = train_data['target']\n",
    "\n",
    "# Split the training set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set grid search parameters\n",
    "parameters = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}  # Regularization strength parameter\n",
    "\n",
    "# Create a Ridge regression model instance\n",
    "ridge = Ridge()\n",
    "\n",
    "# Create a grid search instance with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(ridge, parameters, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params_ridge = grid_search.best_params_\n",
    "best_model_ridge = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions and evaluate performance on the validation set using the best model\n",
    "y_pred_ridge = best_model_ridge.predict(X_val)\n",
    "mse_ridge = mean_squared_error(y_val, y_pred_ridge)\n",
    "print(f'Best Parameters: {best_params_ridge}')\n",
    "print(f'Mean Squared Error: {mse_ridge}')\n",
    "\n",
    "# Retrain the model on the entire training set\n",
    "best_model_ridge.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "In this section, we will analyze the results of the Ridge regression model applied to the financial dataset. The main focus will be on evaluating the model's performance and interpreting its findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "residuals = y_val - y_pred_ridge\n",
    "plt.scatter(y_pred_ridge, residuals, alpha=0.5)\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted values')\n",
    "plt.hlines(y=0, xmin=y_pred_ridge.min(), xmax=y_pred_ridge.max(), colors='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual plot displays the relationship between the differences (residuals) between actual and predicted values and the predicted values themselves. In an ideal residual plot, if the model's predictions have no systematic errors, the residuals should be randomly distributed around the zero line (the red line) without any clear patterns.\n",
    "\n",
    "Most of the residuals are clustered around the zero line, but there seems to be larger fluctuations in residuals within certain ranges of predicted values. This may indicate that the model's predictive performance is not very stable in those specific regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Q (Quantile-Quantile) plot is used to compare the actual distribution of data with a theoretical distribution, typically a normal distribution in regression analysis. This plot is generated by comparing the quantiles of sample data with the quantiles of a theoretical distribution.\n",
    "\n",
    "In a perfect normal distribution, these points should fall on the red diagonal line, which represents the theoretical distribution. The points shown in the plot deviate from this line, especially at both ends, indicating that the data distribution has heavy tails and does not follow a perfect normal distribution.\n",
    "\n",
    "Interpreting the shape of the Q-Q plot:\n",
    "\n",
    "- If the data points are mainly near the line (the red line), it suggests that the data distribution is close to a normal distribution.\n",
    "- If the data points deviate from the line at both ends, especially at the extreme quantiles, it indicates that the data distribution has heavy tails, implying the presence of many outliers or extreme variations.\n",
    "\n",
    "In your Q-Q plot, the data points are relatively close to the line in the middle quantiles but deviate noticeably at both ends, indicating a heavy-tailed distribution. This suggests that the residuals of the model are not completely normally distributed, especially in the extreme values of the data, and the model's performance may deteriorate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Prediction Errors\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of prediction errors, which displays the distribution of model residuals (prediction errors). Histograms are a common tool for visualizing the distribution of statistical data and help us understand the dispersion and central tendency of values in a dataset.\n",
    "\n",
    "In an ideal scenario, if a regression model's predictions are perfectly accurate, all residuals should be close to zero. Additionally, if the residuals are random errors, we expect them to exhibit a shape close to a normal distribution. This means that the histogram should have a bell-shaped curve centered around zero.\n",
    "\n",
    "From the provided histogram, it can be observed that the residuals are primarily concentrated around zero, indicating that the model has made relatively accurate predictions for most of the data. However, the graph exhibits a peak-like shape rather than the typical bell-shaped normal distribution. This may suggest that the residual distribution has some peakedness, which implies that there are many predictions that are very close to the actual values, but it could also indicate the presence of some large prediction errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Drawbacks:**\n",
    "\n",
    "1. The model may be too simplistic: It may fail to capture all the relevant information and patterns in the data, leading to patterns in the residuals.\n",
    "\n",
    "2. Heteroscedasticity may be present: The distribution of residuals varies with changes in predictions, indicating that the model has different predictive abilities in different regions of the data.\n",
    "\n",
    "3. Nonlinear relationships may not have been captured: Linear models may oversimplify and may not adapt well to the actual complexity of the data.\n",
    "\n",
    "4. Influence of outliers or leverage points: The heavy-tailed distribution of residuals suggests that the model may make inaccurate predictions for outliers or extreme cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "far_price_median = train_set['far_price'].median()\n",
    "near_price_median = train_set['near_price'].median()\n",
    "\n",
    "# 定义整体预处理函数，包括两个阶段的预处理\n",
    "def pre_process_ridge(df, scaler):\n",
    "    # 如果存在 'currently_scored' 特征，删除它\n",
    "    if 'currently_scored' in df.columns:\n",
    "        df.drop('currently_scored', axis=1, inplace=True)\n",
    "        \n",
    "    # 使用训练集中位数填充 'far_price' 和 'near_price' 的缺失值\n",
    "    df['far_price'].fillna(far_price_median, inplace=True)\n",
    "    df['near_price'].fillna(near_price_median, inplace=True)\n",
    "\n",
    "    for col in ['imbalance_size']:\n",
    "        upper_limit = df[col].quantile(0.99)\n",
    "        df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
    "\n",
    "    for col in ['imbalance_size']:\n",
    "        df[col + '_log'] = np.log(df[col] + 1e-9)\n",
    "\n",
    "    #df.drop(['row_id', 'time_id', 'date_id'], axis=1, errors='ignore', inplace=True)\n",
    "    df.drop(['row_id', 'time_id'], axis=1, errors='ignore', inplace=True)\n",
    "    \n",
    "    # 特征构建\n",
    "    # 创建买卖价差特征\n",
    "    df['bid_ask_spread'] = df['ask_price'] - df['bid_price']\n",
    "    # 创建买卖不平衡方向与大小的交互特征\n",
    "    df['imbalance_flag_size'] = df['imbalance_buy_sell_flag'] * df['imbalance_size_log']\n",
    "    \n",
    "    # 标准化\n",
    "    continuous_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    continuous_features = continuous_features.drop(['imbalance_buy_sell_flag', 'stock_id'])\n",
    "    #使用训练特征预处理使用的scaler做相同的标准化缩放\n",
    "    df[continuous_features] = scaler.transform(df[continuous_features])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optiver2023\n",
    "\n",
    "# 假设您已经定义了pre_process函数和best_model_ridge模型\n",
    "\n",
    "# 初始化Kaggle环境\n",
    "env = optiver2023.make_env()\n",
    "# 迭代测试集\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "# 对于测试集中的每个批次\n",
    "for (test_df, revealed_targets, sample_prediction_df) in iter_test:\n",
    "    # 对测试数据进行预处理\n",
    "    # 注意: 不要在这里拟合任何预处理步骤，只进行变换\n",
    "    #print(test_df)\n",
    "    \n",
    "    X_test = pre_process_ridge(test_df, scaler)\n",
    "    \n",
    "    # 使用模型进行预测\n",
    "    y_pred = best_model_ridge.predict(X_test)\n",
    "    \n",
    "    # 填充预测结果到sample_prediction_df\n",
    "    sample_prediction_df['target'] = y_pred\n",
    "    \n",
    "    # 提交预测结果\n",
    "    env.predict(sample_prediction_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pre_process_ridge(temp, scaler)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_ridge(test_df, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "\n",
    "After using logistic regression, the results were not very good, so we introduced LightGBM algorithm based on the following characteristics of this algorithm:\n",
    "There are several reasons for choosing to use LightGBM instead of logistic regression:\n",
    "\n",
    "1. LightGBM can process large scale data efficiently. In the context of this competition, financial market data is usually huge.\n",
    "\n",
    "2. It excels at dealing with non-linear relationships. Compared to logistic regression (a linear model), LightGBM (a tree-based model) can better capture the nonlinear relationship between variables, which is crucial for predicting complex financial market movements.\n",
    "\n",
    "3. It has an efficient learning ability. LightGBM uses a gradient-based optimization algorithm and is able to learn patterns in the data much faster.\n",
    "\n",
    "Overall, LightGBM has advantages when dealing with large-scale and complex datasets, and is especially suitable for prediction tasks in the financial domain, such as price prediction in the stock market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the limited computing power of the notebook, we tuned the model parameters locally for efficiency, and finally selected the best set of parameters, see attachment1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, the test set did not contain the test set results, which could not be used to evaluate the quality of the model trained each time. Therefore, in order to reuse the same validation set in different experiments and ensure the consistency of experiments, we divide the validation set from the training set and save it as a separate file.\n",
    "At the same time, we used scatter plot and residual plot to analyze the predicted value and the actual value.\n",
    "The importance of each feature can be seen from the feature importance map. Higher bars indicate that this feature is more important for the model prediction. We can see that matchde_size,seconds_in_buckets,bid_price, and ask_price are the final parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define preprocessing function\n",
    "def pre_process_LGBM(df):\n",
    "    # Avoid division by zero\n",
    "    df['matched_size'] = df['matched_size'].replace(0, 1e-9)\n",
    "    df['bid_size'] = df['bid_size'].replace0(0, 1e-9)\n",
    "    df['ask_size'] = df['ask_size'].replace(0, 1e-9)\n",
    "\n",
    "    # Create new features\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n",
    "    df['imbl_size1'] = (df['bid_size'] - df['ask_size']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['imbl_size2'] = (df['imbalance_size'] - df['matched_size']) / (df['imbalance_size'] + df['matched_size'])\n",
    "    return df\n",
    "\n",
    "# Load the preprocessed training dataset\n",
    "data = pd.read_csv('/kaggle/working/train_modified.csv')\n",
    "data.drop(['bid_ask_spread', 'imbalance_flag_size'], axis=1, inplace=True)\n",
    "\n",
    "# Apply preprocessing function\n",
    "data = pre_process_LGBM(data)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Fill or remove NaN values\n",
    "X.fillna(X.median(), inplace=True)\n",
    "y.fillna(y.median(), inplace=True)\n",
    "\n",
    "# Split the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM model with preset parameters\n",
    "best_LGBM = lgb.LGBMRegressor(num_leaves=70, learning_rate=0.1, feature_fraction=1.0, bagging_fraction=1.0, bagging_freq=5, metric='rmse', verbose=-1)\n",
    "best_LGBM.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_LGBM.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot - Predicted vs Actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs Actual Values')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Reference line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_test, y=y_pred, lowess=True, color=\"g\")\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The residual plot\n",
    "\n",
    "Compared with linear regression model, it shows that the prediction accuracy is better.\n",
    "\n",
    "The distribution of the residual indicates that it may not fully capture all complex patterns in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': X.columns})\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance plot\n",
    "\n",
    "'matched_size' and 'seconds_in_bucket' are the most influential factors for the model's predictions\n",
    "\n",
    "'imbalance_buy_sell_flag' and 'imbalance_size_log' have the least influence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比赛结果提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optiver2023\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 定义整体预处理函数，包括两个阶段的预处理\n",
    "def full_preprocess(df, is_train=True):\n",
    "    # 如果存在 'currently_scored' 特征，删除它\n",
    "    if 'currently_scored' in df.columns:\n",
    "        df.drop('currently_scored', axis=1, inplace=True)\n",
    "\n",
    "    # 第一阶段预处理\n",
    "    # 选择数值类型的列来计算中位数，但对于训练集排除目标变量\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if is_train:\n",
    "        numerical_cols = numerical_cols.drop('target')\n",
    "\n",
    "    median_values = df[numerical_cols].median()\n",
    "    df[numerical_cols] = df[numerical_cols].fillna(median_values)\n",
    "\n",
    "    for col in ['imbalance_size']:\n",
    "        upper_limit = df[col].quantile(0.99)\n",
    "        df[col] = np.where(df[col] > upper_limit, upper_limit, df[col])\n",
    "\n",
    "    for col in ['imbalance_size']:\n",
    "        df[col + '_log'] = np.log(df[col] + 1e-9)\n",
    "\n",
    "    df.drop(['row_id', 'time_id', 'date_id'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "    # 标准化\n",
    "    continuous_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if is_train:\n",
    "        continuous_features = continuous_features.drop(['target', 'imbalance_buy_sell_flag', 'stock_id'])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[continuous_features] = scaler.fit_transform(df[continuous_features])\n",
    "\n",
    "    # 第二阶段预处理\n",
    "    df['matched_size'] = df['matched_size'].replace(0, 1e-9)\n",
    "    df['bid_size'] = df['bid_size'].replace(0, 1e-9)\n",
    "    df['ask_size'] = df['ask_size'].replace(0, 1e-9)\n",
    "\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n",
    "    df['imbl_size1'] = (df['bid_size'] - df['ask_size']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['imbl_size2'] = (df['imbalance_size'] - df['matched_size']) / (df['imbalance_size'] + df['matched_size'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# 使用比赛API获取隐藏测试集\n",
    "env = optiver2023.make_env()  # 确保只调用一次\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test_df, revealed_targets, sample_prediction_df) in iter_test:\n",
    "    # 对测试数据应用整体预处理\n",
    "    test_df_processed = full_preprocess(test_df, is_train=False)\n",
    "\n",
    "    # 选择在训练模型时使用的特征\n",
    "    X_test = test_df_processed[X.columns]\n",
    "\n",
    "    # 填充NaN值\n",
    "    X_test.fillna(X.median(), inplace=True)\n",
    "    \n",
    "    # 使用训练好的模型进行预测\n",
    "    y_pred_test = LGBM_model.predict(X_test)\n",
    "    \n",
    "    # 填充预测结果到sample_prediction_df\n",
    "    sample_prediction_df['target'] = y_pred_test\n",
    "    \n",
    "    # 提交预测结果\n",
    "    env.predict(sample_prediction_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
